# -*- coding: utf-8 -*-
"""metrics_new.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1MR5P26dJJDrA-1MO5a-nnqgk2uQQGJ1G
"""

from sklearn.model_selection import StratifiedKFold
from sklearn.metrics import (matthews_corrcoef, roc_auc_score, precision_score,
                             recall_score, f1_score, cohen_kappa_score, accuracy_score)
from sklearn.ensemble import AdaBoostClassifier
from scipy.stats import hmean
import numpy as np
from tqdm.notebook import tqdm
from sklearn.preprocessing import label_binarize
from sklearn.metrics import balanced_accuracy_score
from sklearn.metrics import average_precision_score


def gmean_score(y_true, y_pred):
    return hmean([precision_score(y_true, y_pred, average='weighted'), recall_score(y_true, y_pred, average='weighted')])

def weighted_accuracy(y_true, y_pred):
    acc = accuracy_score(y_true, y_pred, normalize=False)
    return acc / len(y_true)

def metrics_append (X, y, clf):
  metrics = {
    "Precision": [],
    "Recall": [],
    "F1": [],
    "G-mean": [],
    "MMCC": [],
    "Kappa": [],
    "Weighted Accuracy": [],
    #"ROC AUC": [],
    "PR Score": [],
    "Balanced Accuracy": [],
    }

  # Setup for M-fold cross-validation
  skf = StratifiedKFold(n_splits=10, random_state=42, shuffle=True)
  for train_index, test_index in skf.split(X, y):
    X_train_fold, X_test_fold = X.iloc[train_index], X.iloc[test_index]
    y_train_fold, y_test_fold = y[train_index], y[test_index]  # Direct indexing for y if it's a numpy array

    # Initialize and fit your classifier
    # clf = AdaBoostClassifier(n_estimators=30, algorithm="SAMME", random_state=0)
    ##Yoour classifier##
    #clf=MulticlassClassificationOvR(GradientBoostingClassifier())
    clf.fit(X_train_fold, y_train_fold)

    # Predictions and scores
    y_pred_fold = clf.predict(X_test_fold)
    #y_pred_proba_fold = clf.predict_proba(X_test_fold)[:, 1]  # Assuming binary classification

    # Calculate metrics for this fold
    metrics["Precision"].append(precision_score(y_test_fold, y_pred_fold, average='weighted', zero_division=1))
    metrics["Recall"].append(recall_score(y_test_fold, y_pred_fold, average='weighted'))
    metrics["F1"].append(f1_score(y_test_fold, y_pred_fold, average='weighted'))
    metrics["G-mean"].append(gmean_score(y_test_fold, y_pred_fold))
    metrics["MMCC"].append(matthews_corrcoef(y_test_fold, y_pred_fold))
    metrics["Kappa"].append(cohen_kappa_score(y_test_fold, y_pred_fold))
    metrics["Weighted Accuracy"].append(weighted_accuracy(y_test_fold, y_pred_fold))

    if len(np.unique(y)) > 2:
        y_test_fold_binarized = label_binarize(y_test_fold, classes=np.unique(y))

    else:
        y_test_fold_binarized = y_test_fold

    #metrics["ROC AUC"].append(roc_auc_score(y_test_fold_binarized, clf.predict(X_test_fold), average='weighted', multi_class='ovr'))

    metrics["PR Score"].append(precision_score(y_test_fold, y_pred_fold, average='weighted', zero_division=1))

    metrics["Balanced Accuracy"].append(balanced_accuracy_score(y_test_fold, y_pred_fold))

  for metric, values in metrics.items():
    print(f"{metric}: {np.round(np.mean(values), 3)} ± {np.round(np.std(values), 3)}")

  return {metric: f"{np.round(np.mean(values), 3)} ± {np.round(np.std(values), 3)}" for metric, values in metrics.items()}

metrics_all = {
    "Precision": [],
    "Recall": [],
    "F1": [],
    "G-mean": [],
    "MMCC": [],
    "Kappa": [],
    "Weighted Accuracy": [],
    "ROC AUC": [],
    "PR Score": [],
    "Balanced Accuracy": [],
}

X,y = load_dataset('Wine')

le = LabelEncoder()
y = le.fit_transform(y)
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3,
                                                    random_state=42, stratify=y)
# Then, scale the features
scaler = StandardScaler()
X_train = scaler.fit_transform(X_train)
X_test = scaler.transform(X_test)

#LogitBoost
bin_clf = LogitBoost()
mclf_boost_model = MulticlassClassificationOvR(bin_clf)
metrics = metrics_append (X, y, mclf_boost_model)

for metric, values in metrics.items():
  metrics_all[metric].append(values)

#это в цикле записывает все датасеты в файл
import pandas as pd

list_of_names = ['Wine', 'Hayes_Roth', 'Contraceptive_Method_Choice',
                 'Pen-Based_Recognition_of_Handwritten_Digits',
                 'Vertebral_Column', 'Differentiated_Thyroid_Cancer_Recurrence',
                 'Dermatology', 'Balance_Scale', 'Glass_Identification',
                 'Heart_Disease', 'Car_Evaluation', 'Thyroid_Disease', 'Yeast',
                 'Page_Blocks_Classification', 'Statlog_Shuttle', 'Covertype',
                 ]


algorithms = [LogitBoost(), MEBoost(), AdaBoost(), RUSBoost(), GradientBoostingClassifier()]

for data_name in list_of_names:
  X,y = load_dataset(data_name)

  le = LabelEncoder()
  y = le.fit_transform(y)
  X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3,
                                                    random_state=42, stratify=y)
  # Then, scale the features
  scaler = StandardScaler()
  X_train = scaler.fit_transform(X_train)
  X_test = scaler.transform(X_test)

  metrics_all = {
    "Precision": [],
    "Recall": [],
    "F1": [],
    "G-mean": [],
    "MMCC": [],
    "Kappa": [],
    "Weighted Accuracy": [],
    #"ROC AUC": [],
    "PR Score": [],
    "Balanced Accuracy": [],
    }

  for bin_clf in algorithms:
    mclf_boost_model = MulticlassClassificationOvR(bin_clf)
    metrics = metrics_append (X, y, mclf_boost_model)
    for metric, values in metrics.items():
      metrics_all[metric].append(values)

  pd.set_option('display.max_colwidth', None)
  df_metrics = pd.DataFrame(metrics_all)
  algorithms_names = ['LogitBoost', 'MEBoost', 'AdaBoost', 'RUSBoost', 'GradientBoostingClassifier']
  df_metrics.insert(0, 'Algorithm', algorithms_names)
  output_csv_path = f'tables/metrics_table_{data_name}.csv'
  df_metrics.to_csv(output_csv_path, index=False)